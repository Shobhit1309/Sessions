------------
AWS session:
------------

AWS REGION
AWS AZ
IAM



R1 - REGION
- AZ1
- AZ2
- AZ3

Minimum = 3
Max = 6

1. Compliance
2. Proximity
3. Available service
4. Pricing


IAM:


ROOT USER
-- usergroups
   -- users

-- users

to set permission and restriction - policy (json document)


Developers: (permission) - p1 
u1
u2
u3

Operations: (permission) - p2
u4
u5
u6

Audit: (permission) - p3
u3
u4

u3 - p1 , p3
u4 - p2 , p3


Basic Admin permission 
- create a user
- modify a user

X 

BAP ( policy BAP : create and modify permission )
X



-------------------

{

 "Version": "2012-10-17",
 "Statement": [
 {
 "Effect": "Allow",
 "Action": "ec2:Describe*",
 "Resource": "*"
 },
 {
 "Effect": "Allow",
 "Action": "elasticloadbalancing:Describe*"
,
 "Resource": "*"
 },
 {
 "Effect": "Allow",
 "Action": [
 "cloudwatch:ListMetrics",
 "cloudwatch:GetMetricStatistics",
 "cloudwatch:Describe*"
 ],
 "Resource": "*"
 }
 ]
}


Json: ( semi structured file )

a={
	"key1" : "value1",
	"key2" : "value2",
	"key3" : "value3",
	.........
	"key4":["val1","val2",....]
	"key5":[{"k1":"v1"},{"k2":"v2"},....]
	"key5":[{"k1":"v1"},{"k2":["v1","v2"]},....]
}


print(a[key1])
value1

a[key1]=value10

print(a[key1])
value10


shobhit
https://767397847830.signin.aws.amazon.com/console




Regions :

Type 1 : Global service. - IAM , 
Type 2 : Region Specific Service - Lambda / Glue / RDS 

compute:
1. server based service - EC2
2. serverless based service - Lambda , Glue ( automatically memory , vcpu - autoscale )


list:
EC2 , Lambda , glue , ECS , 






AmazonS3FullAccess
AmazonDynamoDBFullAccess
AWSGlueServiceRole
AWSLambdaFullAccess
AmazonRedshiftFullAccess
AmazonAthenaFullAccess
AmazonKinesisFullAccess
AmazonEC2ReadOnlyAccess
AWSLambdaExecute
AWSGlueConsoleFullAccess







{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "BaseAthenaPermissions",
            "Effect": "Allow",
            "Action": [
                "athena:*"
            ],
            "Resource": [
                "*"
            ]
        },
        {
            "Sid": "BaseGluePermissions",
            "Effect": "Allow",
            "Action": [
                "glue:GetDatabase",
                "glue:GetDatabases",
                "glue:UpdateDatabase",
                "glue:CreateTable",
                "glue:BatchDeleteTable",
                "glue:UpdateTable",
                "glue:GetTable",
                "glue:GetTables",
                "glue:BatchCreatePartition",
                "glue:CreatePartition",
                "glue:BatchDeletePartition",
                "glue:UpdatePartition",
                "glue:GetPartition",
                "glue:GetPartitions",
                "glue:BatchGetPartition",
                "glue:StartColumnStatisticsTaskRun",
                "glue:GetColumnStatisticsTaskRun",
                "glue:GetColumnStatisticsTaskRuns"
            ],
            "Resource": [
                "*"
            ]
        },
        {
            "Sid": "BaseQueryResultsPermissions",
            "Effect": "Allow",
            "Action": [
                "s3:GetBucketLocation",
                "s3:GetObject",
                "s3:ListBucket",
                "s3:ListBucketMultipartUploads",
        
            "Resource": [
                "arn:aws:s3:::aws-athena-query-results-*"
            ]
        },
        {
            "Sid": "BaseAthenaExamplesPermissions",
            "Effect": "Allow",
            "Action": [
                "s3:GetObject",
                "s3:ListBucket"
            ],
            "Resource": [
                "arn:aws:s3:::athena-examples*"
            ]
        },
        {
            "Sid": "BaseS3BucketPermissions",
            "Effect": "Allow",
            "Action": [
                "s3:ListBucket",
                "s3:GetBucketLocation",
                "s3:ListAllMyBuckets"
            ],
            "Resource": [
                "*"
            ]
        },
        {
            "Sid": "BaseSNSPermissions",
            "Effect": "Allow",
            "Action": [
                "sns:ListTopics",
                "sns:GetTopicAttributes"
            ],
            "Resource": [
                "*"
            ]
        },
        {
            "Sid": "BaseCloudWatchPermissions",
            "Effect": "Allow",
            "Action": [
                "cloudwatch:PutMetricAlarm",
                "cloudwatch:DescribeAlarms",
                "cloudwatch:DeleteAlarms",
                "cloudwatch:GetMetricData"
            ],
            "Resource": [
                "*"
            ]
        },
        {
            "Sid": "BaseLakeFormationPermissions",
            "Effect": "Allow",
            "Action": [
                "lakeformation:GetDataAccess"
            ],
            "Resource": [
                "*"
            ]
        },
        {
            "Sid": "BaseDataZonePermissions",
            "Effect": "Allow",
            "Action": [
                "datazone:ListDomains",
                "datazone:ListProjects",
                "datazone:ListAccountEnvironments"
            ],
            "Resource": [
                "*"
            ]
        },
        {
            "Sid": "BasePricingPermissions",
            "Effect": "Allow",
            "Action": [
                "pricing:GetProducts"
            ],
            "Resource": [
                "*"
            ]
        }
    ]
}




AWS :
1. Console
2. AWS CLI - command line interface	
3. AWS SDK - Software dev kit

SDK:
1. Inside AWS from any AWS service ( Lambda / Glue / API Gateway ... )
import boto3
gluesession=boto3.session("Glue")

2. local mac --> AWS (python code - boto3)



1. csv file everyday on s3
2. quality check on file
3. passed then you need to run tx on data 
4. transformed data should be laoded in SQL database
5. send email on completion of the process


s3 -- > lambda -- > Glue (python/pyspark) -- > SQL server (EC2/AroraDB/RDS) --> SES


Keys:
Access Keys  - User_id
Secret Keys  - PWD

Key CHANGE once in :
-- 90 days
-- Key rotation mechanism



aws iam list-users --env DEV --region DEV-REGION


DEV 
ak1
sk1

UAT
ak2
sk2

PROD
ak3
sk3



( local terminal ) aws iam list-users
1. config to check for keys
2. get the keys from config and login to aws
3. login successfull then go to iam service
4. run list-users to get list of users
5. return the output as a json string to the local terminal



IAM - ROLES
S3


U1 - Data engineer
U2 - Data engineer

U3 - Data analyst

U4 - Data scientist



ROLES - AWS SERVICE

EC2 --- S3 
    --- Lambda

Apache AIRFLOW 
s3 -- > lambda -- > Glue (python/pyspark) -- > SQL server (EC2/AroraDB/RDS) --> SES

AWS Step function -- orchest





S3:

-- Infinite Scaling service
-- Store the files 
-- Create a data lake ( structed , unstructed , semi-)
-- Create a data catalog
-- Disaster Recovery
-- Database backups
-- Version control

-- Store the code of your website

S3:
1. Buckets
2. Objects (files)
3. Keys


s3://my-bucket/my-file.txt
s3://my-bucket/my-folder/my-file.txt

Max object size - 5TB 
Object size of 5GB - use multi-part upload
Versioning can be done on S3 files 




Access of S3:

1.  Region specific service -- which user base what's to access the s3 , which service whats to access

Application (Service) Access:
=============================
us-east-1
S3 -- bucket-test/test/test.txt

us-east-1
Lambda (apply necessary policy on bucket or apply IAM role)

us-west-1
Lambda (same policy like us-east-1 lambda)



-----
s3 bucket can be accessed publically without any rule

- no aws users are required
- no policy 
- no IAM


 static websites
 - private
 - public




ACL
-- 
IAM roles + Policies + AWS setting --> access of a user who is accessing the bucket


hospital 

payment ( tx ) POS --> generates the recipt (online) --> event trigger on Lambda --> move the file to AWS S3 --> SNS ( sms to the user )


Cross Account Access:


Client 

AWS
===
1. Acc1 - Sales team is using ( Tech ) -- csv file
2. Acc2 - Data Team (s3)



Versioning:
------------

Bucket level 

S3 --> test.txt ( contents )

Versioning - disabled

test.txt - marker -- null

Versioning - activated

test.txt - marker -- id1 ( random text - version id) - v1
test.txt - marker -- id2 ( random text - version id) - v2 
test.txt - marker -- delete marker - id3 ( random text - version id) - v3 ( hide the file from the user )



Version1:
test.txt -- file -- data
markers -- memory locations on the servers -- metadata of the file

Version2:
test.txt -- file -- data
markers -- memory locations on the servers -- metadata of the file


Version3:
test.txt -- file -- data
markers -- memory locations on the servers -- metadata of the file



 type:
1. file extension
2. delete marker



Versioning:
1. UI
2. AWS CLI - aws s3api put-bucket-versioning --bucket your-bucket-name --versioning-configuration Status=Enabled

Shell script : ( automation - INfra provisioning )

aws create s3 buckets
aws enable versioning
aws object lifecycle policies
aws set s3 permissions

details of operation >> log.txt

aws create ec2 server
aws set ec2 permissions


Cross - Region Replication

s3 -- us-east1 ---> s3 -- us-west1



Use case:
---------
End to end architecture for an e-commerce chain

source --- reporting level

Source
1) batch processing -- s3
2) real time -- Amazon Kinesis data stream ( Kafka stream pub sub)

data storage
1) structured - table - RDS , Arora , Redshift ( datawarehouse ) ...
2) semi-structured - json -- Dynamodb (NOSQL database)
3) semi-structured,structured,unstructured -- S3 ( Datalake )

data processing
1) Real time -- Lambda ( triggering workflows , QC of the data ) -- 15 mins
2) Big data application - EMR ( Elastic Map Reduce -- Hadoop setup , spark setup )
3) ETL work -- size -- midsize ( 1-50 GBs ) -- Glue ( Python , Pyspark )

Data analytics
1) Redshift - Datawarehouse
2) Athena - for running queries on S3 data 






Topics for today:
--------------------
1. Data replication on S3 
2. Storage classes in S3
3. Insurance use case , data model and architecture design




Data rep :
1. Cross Region Replication ( CRR )

bucket1 = us-east-1 (P) 
bucket2 = us-west-1 (S)
CRR - (P)

-- Versioning on the buckets (P & S) must be activated


case 1: Rep is ON / ver is OFF ( Hypothetical )

test.txt - bucket1 -- 1234
test.txt - bucket2 -- 1234

test.txt - bucket1 -- not maintain the history -- 123456
test.txt - bucket2 -- not maintain -- 123456

delete test.txt -- bucket 1 -- not maintain the history - file removed
-- AWS will stop this replication action to happen (X) ( Security feature )

rm -f <filename>
rm -f *


case 2 : Rep is ON / ver is ON ( Real implementation )
delete test.txt -- bucket 1 --  maintain the history - delete marker - file removed ( file is still there )
delete test.txt -- bucket 2 --  maintain the history - delete marker - file removed ( file is still there )
rm -f *
RESTORE OPERATION

S3 -- TOGGLE 

V1 -- NEW FILE  -- VERSION_ID:1 : TYPE: NULL
V2 -- FILE UPDATED -- VERSION_ID:2 TYPE: NULL
V3 -- FILE DELETED -- VERSION_ID: 3 TYPE : DELETE MARKER

2. Same Region Replication ( SRR )

bucket1 = us-east-1 (P) - 10 files (test_yymmdd.csv)
bucket2 = us-east-1 (S) - 1 file   (test_yymmdd.csv) 



Chain operation -- NOT supported in Replication

bucket 1 -- bucket 2 -- bucket 3 
(Rep-rule1) - (Rep-rule2) 

test1.txt -- test1.txt -- X


bucket 1 -- bucket 3 
(Rep-rule1)
test1.txt -- test1.txt


-------

Bucket 1 -- Bucket 2 -- Bucket 3

 ( source ) ( Inter) ( Dest )


CASE1:
------

Bucket 1     -- Bucket 2 -- Bucket 3
(rule1)
(copy 1-2)
test.txt     -- test.txt
(orginal-abc1)   (copy - xyz-1)


CASE2:  NO CHAIN REPLICATION IS POSSIBLE 
-------
        Bucket 1     -- Bucket 2    --   Bucket 3
        (rule1)         (rule2)
        (copy 1-2)      (copy 2-3)

(a)     customer.txt.   --  customer.txt --   X
        (original file)    (copied file)
         version id -abc1  version id -xyz-1


b)      Bucket 1     -- Bucket 2 
        Bucket 1     -- Bucket 3 




AWS
-- 2 accounts 



MICROSOFT (AWS)
-- FIN ( AWS Seperate Account - AID 1 )
-- TEC ( AWS Seperate Account - AID 2 )
-- SALES ( AWS Seperate Account - AID 3 )
-- HR ( AWS Seperate Account - AID 4 )


Dataset -- Survey

Technology tool which customer is using 
Happy with the price
Happy with the after sales services 

(csv file )

Cross account replication


file1 --> Sales-bucket ( Sales account AID 3 ) ---> tech-bucket ( tech - account - AID 2 )
                ( Replication rule )                     ( Replication rule )



AWS Key Management System  -- Key

Customer -- TX -- CC -- Invoice data file --- AWS 



Bucket 1          --         Bucket 2
Replication Rule 
(Replication Job)




Batch Operation
--------------

Bucket 1   

- file1
- file2
- file3
- file4 
20th Apr 2023

21th Apr 2023 -- ( Replication Rule ) -- do you want to move the Existing file or not ?
- file5
- file6
- file7

Bucket 2  ( data analytics BA )
- file1
- file2
- file3
- file4 

- file5
- file6
- file7

Batch Operation - will move historical data before the creation of replication rule to destination bucket


--------------------
 S3 storage classes:
--------------------
- data availabilty 
- durability of the data
- cost of storage the data
- data retrieval cost


==========
Insurance:
==========

1. Usecase
2. ODS --> DW on AWS vs Onprim



Source:
--------
1. Flat files ( csv , tsv , xls )


Policy data -- hold by the insurance company
Claims data - claims processing data -- insurance company + third party policy generation company
CRM ( customer relationship management data ) -- customer profiles , customer interaction with Agents , customer feedback and survey data
                                                                                                                ( Qualtrics ) audio recording + 
                                                                                                                 feedback form + tran script 

                                                                                                                   REST API --> Python based API
                                                                                                                   ( requests ) --> hit API --> Extract data-- > Json




Sources:
1. Files - Policy Data 
2. Rest APIs - Feedback data
3. Databases - Claims data


source ---> S3 ( landing layer )












SOAP API: 
XML file

REST API:
1. Request: {"company_name":"ABC","duration":"last1 month"}
2. Response - Json format

Python 3
import requests 





 
{
    "name":"shobhit"
    "address":"176 XYUS"
    "age":49
}    


[
   {
    "client_name":"shobhit",
    "client_policy_num": 3872197871289,
    "claim_date": "12-09-2002",
    "first_claim_done":True
    },
    {
    "client_name":"john",
    "client_policy_num": 38721978712892,
    "claim_date": "12-09-2002",
    "first_claim_done":True
    }
]

[
{
"client_name":["shobhit","john"...]
"client_policy_num": [3872197871289,972137147021]
"claim_date": ["12-09-2002",]
"first_claim_done":[True,False]
}
]




for elements in var:
    for phone_numbers in phone_numbers:
        if phone_numbers[type]='home':
            number=phone_numbers[numbers]
        else:
            pass


var = [
{
  "person": {
    "name": "John Doe",
    "age": 30,
    "address": {
      "street": "123 Main St",
      "city": "Anytown",
      "state": "CA",
      "zip": "12345"
    },
    "phone_numbers": [
      {
        "type": "home",
        "number": "555-555-5555"
      },
      {
        "type": "work",
        "number": "555-555-5556"
      }
    ],
    "is_active": true,
    "hobbies": [
      "reading",
      "hiking",
      "coding"
    ]
  }
},
]



person:
  name: John Doe
  age: 30
  address:
    street: 123 Main St
    city: Anytown
    state: CA
    zip: 12345
  phone_numbers:
    - type: home
      number: 555-555-5555
    - type: work
      number: 555-555-5556
  is_active: true
  hobbies:
    - reading
    - hiking
    - coding







ODS :
====


source -->  ODS ( 1 weeks of data in ODS - Reporting on OLTP database ) ---> PUSHED WEEKLY (overnight batch) ---> PUsh to DW ( Reporting on  database OLAP )


ODS
( Medallian Architecture )

Bronze Layer --         Silver Layer --             Gold Layer 
( RAW DATA ) ( Cleaned data + BL applied ). ( Data rolled up or Aggrigated on the business KPIs or KBQs )

Truncate Load

stg_claims_proc
stg_policy
stg_survey

                            dim_policy (pk)
                            dim_customer (pk)
                            dim_dates (pk)
                            dim_territory (pk)
                            fact_claims_process (fk)    report_claims (fk , pk , sk )
                                                        report_surveys (fk , pk , sk )
                                                        report_policy_sales (fk , pk , sk )




10000 - each territory 

50 territory 

10000x50 = 500000 record




2 use cases:
-------------
a) E-commerce data processing on AWS and Reporting
b) Processing the user survey data , analysed  and report the user sentiments on the dashboard plus a self service chat interface powered by OpenAI gpt-4 model which will use user survey data.

1. What data volume we can expect to flow into the system and what is going to freq ( everyday , weekly , month ) ?
a) Database records 
b) Files
c) API data from source


>>> Storage service 
    - which type of files ( structure , semi-structure , unstructure )
                            SQL database , NO SQL database , SQL/NOSQL 
                            
                             raw 
                            (s3)

>>> Processing service 
 1) volume 
 2) Freq
 3) Business logic

 a) light weight process - lambda function ( 15 min )
 b) Medium process - Glue
 c) Heavy process - Split data into multiple streams ( process parallely )
                    -- EC2 instance ( cloudera )
                    -- EMR (elastic map reduce - Hadoop )



Data  --> Data science ( analyse data , create model , model training , store the predicted output )
      --> Reporting ( BI reports )



a) E-commerce data processing on AWS and Reporting

a) Real-time streaming data ( stream of data ) - POS machine (8am - 10pm)
b) Batch data (12am)

--------
Storage:
--------
Before Processing:
RAW -- S3 ( files )
(structure , semi-structured)

After processing:
Structure - SQL ( RDS , Arora , Redshift )
Semi-Structure - Json : Dynamodb database 
S3 in the form of files 

S3:
source -- ecommerce_sales/raw/cust_transaction_yyyymmdd.csv
processed data -- ecommerce_sales/processed

----------------
Processing:
----------------

s3 --> lambda function ( event trigger ) --> Call glue job using boto3 module --> [GLUE JOB]    
       (python,nodejs,Golang,Javascript).     (glue = boto3.client('glue'))        -- Business logic application 
                                                                                      on the data
      
       Quality Checks on the data
       -- file shouldn't be 0 byte
       -- file should have correct headers
       -- duplicate user_ids
       -- transaction_id should not be null


Lambda:
-------
import boto3
glue=boto3.client('glue')
response = glue.start_job_run(JobName='ecommerce_process')



-- Orchestration ( DAG )
   -- Step function 


box1
return code - 1
 |
 v
box2



{
    "name":"shobhit",
    "data":{"this is a test data"}
}

{
    "name":"shobhit",
    "data":{}
}


if a==b:
    print("a=b")
else:
    pass





{
    "survey_id":197309-17091-7309,
    "survey_type":"textual survey",
    "question1":"answer1",
    "question2":"answer2",
    ...........
    ...........
}

0. check the schema of the data and clean data to make it standardized
1. convert the json into row column data -- pandas.    (  )
2. connect to database , insert the data in the table

RDS

survey_data

survey_id, survey_type , questions , answers , insert_dt




import pandas as pd
df = pd.DataFrame(data)


import boto3

session=boto3.session("RDS")


python
service 1 -- service 2






import pymysql
import boto3
import os
import json
import pandas as pd


rds_host= os.environ['RDS_HOST']
name=os.environ['DB_NAME']
password = os.environ['PASSWORD']


conn = pymysql.connect(host=rds_host,user=name,passwd=password)

df = pd.DataFrame(data)

with conn.cursor() as cursor:
for records in df:
        cursor.execute("INSERT into survey_data values (df['survey_id'],df['survey_name'],df['questions'],df['answers']")
        cursor.commit()


cursor.close()
conn.close()





OPENAI
-- GPT4 turbo ( accepts text and responses are also in text )
-- Omni model ( GPT 4-o ) 
-- DALLE ( generate image )

1. UI -> PROMPT -> OUTPUT
2. Using REST APIs



sample survey data:
------------------

data = [
{
    "survey_id":197309170917309,
    "survey_type":"textual survey",
    "question1":"answer1",
    "question2":"answer2",
    ...........
    ...........
},
{
    "survey_id":192309170917312,
    "survey_type":"textual survey",
    "question1":"answer1",
    "question2":"answer2",
    ...........
    ...........
}
]

-------------
Databricks:
-------------


1.  Lakehouse platform in databricks
2.  Unity Catalog
3.  Delta tables




Databricks  -- spark ( processing )
            -- other opensource technologies ( Delta lake )
            -- OSS Unity Catalog
            -- ML flow 


Lakehouse - Data lake + Data warehouse

ACID transactions 

source --> s3 --> DBFS mounted on S3 --> Databricks tables ( parquet format in the backend )
                                         select * from table ;


Onprim: Datawarehouse -- License , Storage + Compute , maintainance of datawarehouse -- Cost is high

AWS: Redshift (MPP) - Datawarehouse -- Storage + Compute -- Cost is high


s3 -- Lambda (15 min) -- glue -- database -- reportingtool 
s3 -- glue -- database -- reportingtool 
      (service is running)

spin-up : creating/ configuring the servers to setup a service 

Databricks 
----------   + proton acceleration 
  spark



Lakehouse -- Delta tables ( normal table + time travel )

customer - customer_id , customer_name , address 

create table customer
(
customer_id int,
customer_name string,
address string
) USING delta;

versioning or version number 
4 -- 3 

CDC table

1- data load (new record)
2- data load (new record)
3- data load (new records + existing updates)
4- data load (new record)


Unity Catalog:
--------------
Cataloging -- metadata of tables , applying some permissions on the tables for authorized use , maintaining the schema of data and also schema enforcement , data linege 

1. hive_metastore
2. unity catalog


bronze_
raw_

silver_
stage_

gold_
final_
reporting_
rpt_
